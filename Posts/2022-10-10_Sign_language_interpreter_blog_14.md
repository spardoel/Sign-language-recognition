# Developing a sign language interpreter using machine learning Part 14 - Implementing the new holistic cropping approach

In the previous post I talked about how I reworked my appraoch for the program. In the previous version, I used the YOLO model to identify the person in the frame, then running the holistic feature extractor model on the cropped frames. 
In the new version I removed the YOLO model completely. The holistic feature extractor model is run on each frame as soon as it is available. Then, once a clip is ready for processing, the video is cropped and the features are resized to match the cropped frame. 

In this post I will go through the code and talk about how I implemented the new methods. Originally I used pandas dataframes for storing the values for each frame. After some testing I found that the dataframes were slowing things down. 
In a few places in the code, converting the data to numpy arrays was faster. In the end I still used dataframes to pass variables between functions, but when processing speed was important, the data were coppied to numpy arrays immediately before processing. 
This was my compromise between speed and simplicity. I'll talk a bit more about that later. 

## Class overview

Here are the 3 main classes used in the real-time sign recognition program, namely, the VideoCamera class, the FeatureExtractor class and the Data class. 


![three primary classes](https://user-images.githubusercontent.com/102377660/194890568-daab5142-ba74-4a6e-9c17-eb66feb08dcd.png)



In the next section I will go through each class and talk about the primary methods. Note that some of the helper methods may be changed in the final version of the code. 
My goal for this post is to talk through my method, the exact code is subject to change. 

## The Video Camera class

Alright, since this program is being made to run on a live webcam feed, I needed some functions to handle getting new frames from the webcam.
This class is pretty simple, it has a CV2 object that controls the webcam. It also has a function to get the next frame when needed. That's it. 

## The Data class

My goal for the FeatureExtractor class was to streamline the processing of each frame, then dump the processed data into queues. The Data class was made to handle these queues.

For the Data class, the properties are simply 5 circular queues. One for each of the left hand, right hand, and pose coordinates. As well as a queue for the bounding boxes adn the video frames themselves. 

The __init__ method created the queues with a specific size. The classification model was trained on 2 second clips of video, so the default queue size is 50 frames (at 25 frames per second). 

The methods are pretyy simple also. add_new_frame() simply appends the data from the current frame to the end of the circular queue. get_queue_tail() returns the value of the tail of one of the queues. Since the queues are synchronized, they all have the same tail values. This method is used to know when the queue is full and when all values have been overwritten and an entierly new clip is stored in the queues. get_clip() returns the entire contents of all queues. This is called when it is time to process the clip. Finally, write_video_to_file(), saves a given video clip. 


## The FeatureExtractor class

Alright, deep breath. This is the class that holds all the real functionality of the program.

Let's start with the properties. The class has the holistic feature extraction model as its first property. No surprise there. 

Next, there are the pandas dataframes that are used to store the coordinates of the points generated by the holistic model. As mentioned in a previous post, the hands each have 21 points, and each point has an X,Y, and Z component. So the shape of these dataframes is 21,3. Similarly, the pose estimator which includes the eye, shoulder, and arm points, has a total of 23 points. 
Each with an X and a Y component. The dataframe used to store these points is of size 23,2. 
These dataframes are one dimensional meaning that they can store the data for a single frame only. To store the data for multiple frames, the Data class is used.

The bounding_box property holds the coordinates of the 2 points that define the bounding box for each frame. The frame width and frame height are also saved as properties. 

### The methods

There are many methods within the FeatureExtractor class. I will talk through the logic behind the most important methods, but I won't necessarily copy paste all of the code. My goal for this section is to explain the flow of data through the program and the logic behind each processing step. 

The __init__ method sets the hand and pose point properties to zeros. This method also defines a few important values such as the maximum number of frames per clip (50 frames).

run_feature_extractor_single_frame() is the next method on the list. This method is called for each new frame of data. The method accepts the frame (image) from the VideoCamera() class and runs the holistic model on the frame. Then, the extract_coordinates_from_results() method is called to unpack the results and save them in the left_hand, right_hand, and pose properties. Note that this method also multiplies the percentage values by the image dimensions such that the coordinates of the points are now in pixels (see the diagrams in the previous post for details about the coordinate normalization process). Then, get_single_frame_bounding_box() is called to identify the bounding box that encompasses all of the points held in the left_hand, right_hand, and pose properties. The last step in the run_feature_extractor_single_frame() method is to save these values in the Data() class. The Data() class adds the values to the end of the queues. 

To sumarize, run_feature_extractor_single_frame() runs the image through the holistic model, estracts the coordinates of the points of interest, identifies the bounding box, then saves the values for the current frame in the Data() class queues. 

For reference here is the code 
```
    def run_feature_extractor_single_frame(self, image, data_structure):

        # reset the variables to zeros
        self.left_hand = pd.DataFrame(np.zeros((21, 3)), columns=["x", "y", "z"])
        self.right_hand = pd.DataFrame(np.zeros((21, 3)), columns=["x", "y", "z"])
        self.pose = pd.DataFrame(np.zeros((23, 2)), columns=["x", "y"])

        # get the shape of the input image
        self.input_image_size_y, self.input_image_size_x, _ = image.shape
 
        # run the feature extractor model on the image
        image.flags.writeable = False
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        results = self.holistic.process(image)
        
        # unpack the results
        self.__extract_coordinates_from_results(results)
        self.__get_single_frame_bounding_box()
        
        # add the frame data to the queues in the Data() class
        data_structure.add_new_frame(
            image, self.bounding_box, self.left_hand, self.right_hand, self.pose
        )
```

Next, I want to talk about the 'process_clip()' method. This is where things start to get complicated. This method is called when clip frames have been processed and it is time for the classification model to guess the sign in the video. 

Here is a simplified version of the code.

```
    def process_clip(self, data_structure):

        (
            frames,
            boxes,
            LH_coordinates,
            RH_coordinates,
            P_coordinates,
        ) = data_structure.get_clip()

        # Find the bounding box min and max values
        self.__get_bounding_box_min_max(boxes)
        self.__check_bounding_box_validity(frames[0])

        # Crop the clip to the bounding box
        clip_cropped = frames[
            :,
            int(self.bounding_box.y_min[0]) : int(self.bounding_box.y_max[0]),
            int(self.bounding_box.x_min[0]) : int(self.bounding_box.x_max[0]),
            :,
        ]

        # loop through the frames
        self.output_features = np.zeros((len(frames), self.num_features))
        ouput_clip = []

        for fr_idx, frame in enumerate(clip_cropped):

            frame = cv2.resize(frame, (350, 350))

            # set the variables to zeros to prevent cary over from previous frame
            self.left_hand = pd.DataFrame(np.zeros((21, 3)), columns=["x", "y", "z"])
            self.right_hand = pd.DataFrame(np.zeros((21, 3)), columns=["x", "y", "z"])
            self.pose = pd.DataFrame(np.zeros((23, 2)), columns=["x", "y"])

            self.__map_coordinates_to_resized_image(
                fr_idx,
                LH_coordinates[fr_idx],
                RH_coordinates[fr_idx],
                P_coordinates[fr_idx],
            )

            ouput_clip.append(frame)

        frame_masks = np.ones(shape=(len(frames)), dtype="bool")

        # reset the bounding box to zeros after processing the clip.
        self.bounding_box = pd.DataFrame(
            np.zeros((1, 4)), columns=["x_min", "y_min", "x_max", "y_max"]
        ).astype(int)

        return self.output_features, frame_masks, ouput_clip
```

The method starts by getting the video, lists of coordinates, and list of bounding boxes from the 'Data()' class. Then the 'get_bounding_box_min_max()' method is called. This method accepts the list of bounding boxes (one for each frame) and finds the largest bounding box. 

Each bounding box is defined by 2 points. The upper left point, which is closest to the origin and has the (xmin,ymin) components, and the lower right point with the (xmax,ymax) components. The 'get_bounding_box_min_max()' method finds the minimum and maximum values of the X and Y components separately. Once the xmin,ymin,xmax,ymax values are found, the 'check_new_bounding_box_values()' method is called. This method checks that the values are not nan, and assignes the values to the bounding box class property. 

Next, once the single bounding box for the clip has been found, the 'check_bound_box_validity()' method is called. This method checks that the bounding box is not larger than the dimensions of the input frame.

Once the bounding box for the video has been found and checked, slice notation is used to crop the video clip. 

Next, the code loops through each frame of the video, and re-sizes the holistic points to match the new, cropped video using 'map_coordinates_to_resized_image()'. Remember, that the coordinates are currently in pixels because the coordinates of the points were converted when the frame was originally processed. As I was saying, the code loops through each frame. The frame is resized to be 350 by 350 pixels. Then the 'map_coordinates_to_resized_image()' method is called. This method is where the normalization happens.  

After the features are re-normalized to the cropped image size, the mask output variable is prepared. The mask variable is set to zeros, then the length of the video clip is used to set some of the values to ones. Previously, the masks were not used because it was assumed that every video clip would be exactly 50 frames long. The classification model is expecting a 50 frame input, so the masks were not needed. However, after some testing, I found that 2 seconds is a long time between different words. When using the real-time model I would often sign a word then have to wait for the program to finish collecting the rest of the 50 frames. This was not a problem in terms of functionality, but it made the program feel slugish. By using the mask variable here to identify the frames of video, I can change the number of input frames between classifications. I'll talk more about that a bit later. 

Finally, the bounding box property is reset to zero, and the features, masks, and the resized video clip are returned. 

### Re-normalizing the coordinates. 

In the previous section I mentioned that the 'map_coordinates_to_resized_image()' method is used to re-normalize the feature values. In this section, I want to take a closer look at this method.

As I mentioned previously, the coordinates of the points returned from the holistic model are represented as fractions of the image size. When the individual frames are processed, the coordinates are converted to pixel values. The 'map_coordinates_to_resized_image()' method converts the points back to percentage values after the video frames have been cropped. 

Here is the code.

```
 def __map_coordinates_to_resized_image(self, fr_idx, left_hand, right_hand, pose):

        # Values are converted to numppy arrays for faster processing. 

        # Left hand points 
        LH_x = np.asarray(left_hand.x)
        LH_y = np.asarray(left_hand.y)
        LH_z = np.asarray(left_hand.z)

        for loop_idx in range(len(LH_x)):
            if LH_x[loop_idx] > 0:
                LH_x[loop_idx] = (
                    LH_x[loop_idx] - self.bounding_box.x_min[0]
                ) / self.new_size_x
                LH_z[loop_idx] = (
                    LH_z[loop_idx] - self.bounding_box.x_min[0]
                ) / self.new_size_x
            if LH_y[loop_idx] > 0:
                LH_y[loop_idx] = (
                    LH_y[loop_idx] - self.bounding_box.y_min[0]
                ) / self.new_size_y


        # Right hand points
        RH_x = np.asarray(right_hand.x)
        RH_y = np.asarray(right_hand.y)
        RH_z = np.asarray(right_hand.z)

        for loop_idx in range(len(RH_x)):
            if RH_x[loop_idx] > 0:
                RH_x[loop_idx] = (
                    RH_x[loop_idx] - self.bounding_box.x_min[0]
                ) / self.new_size_x
                RH_z[loop_idx] = (
                    RH_z[loop_idx] - self.bounding_box.x_min[0]
                ) / self.new_size_x
            if RH_y[loop_idx] > 0:
                RH_y[loop_idx] = (
                    RH_y[loop_idx] - self.bounding_box.y_min[0]
                ) / self.new_size_y

        # Pose points
        P_x = np.asarray(pose.x)
        P_y = np.asarray(pose.y)

        for loop_idx in range(len(P_x)):
            if P_x[loop_idx] > 0:
                P_x[loop_idx] = (
                    P_x[loop_idx] - self.bounding_box.x_min[0]
                ) / self.new_size_x
            if P_y[loop_idx] > 0:
                P_y[loop_idx] = (
                    P_y[loop_idx] - self.bounding_box.y_min[0]
                ) / self.new_size_y


        # flatten the data in the order x,y,z. Store in new, 1-D arrays
        LH_output = np.zeros(shape=(len(LH_x) * 3))
        for loop_idx in range(len(LH_x)):
            LH_output[3 * loop_idx] = LH_x[loop_idx]
            LH_output[3 * loop_idx + 1] = LH_y[loop_idx]
            LH_output[3 * loop_idx + 2] = LH_z[loop_idx]

        RH_output = np.zeros(shape=(len(RH_x) * 3))
        for loop_idx in range(len(RH_x)):
            RH_output[3 * loop_idx] = RH_x[loop_idx]
            RH_output[3 * loop_idx + 1] = RH_y[loop_idx]
            RH_output[3 * loop_idx + 2] = RH_z[loop_idx]

        P_output = np.zeros(shape=(len(P_x) * 2))
        for loop_idx in range(len(P_x)):
            P_output[2 * loop_idx] = P_x[loop_idx]
            P_output[2 * loop_idx + 1] = P_y[loop_idx]


        # Concatenate the flattened arrays into a single 1-D array
        self.output_features[fr_idx] = np.concatenate(
            (LH_output, RH_output, P_output), axis=0
        )

        # replace any nan values with zeros
        self.output_features[fr_idx][np.isnan(self.output_features[fr_idx])] = 0
```

This method re-normalizes the points for the left hand, right hand, and for the body landmarks (called pose). The code for each of these variables is basically identical. 

Let's start with the left hand points. The methods accepts the pandas dataframe containing the left hand points. The dataframe has 3 columns namely X,Y, and Z. Each point is a row and each point coordinate is held in a separate column. This makes it easy to access the coordinates of a specific point. The problem with using datframes like this is that it is slow. I found that converting the data to numpy arrays was much faster. This is why the code starts by converting the dataframe columns to separate numpy arrays. 

Next, the code loops through each each point. If the value of the coordinate is larger than 0, then subtract the min value that was used to define the cropping bounding box. Then, divide by the new size of the cropped image. This process is explained in greater detail in the previous post. You may notice that the Z coordinate is being normalized using the X values. This is intentional. The holistic model documentation states that the Z coordinate is the estimated distance from the camera and is equivalent to the units of the X direction. Since the images do not have a third (Z) dimension, and because the units are the same as the X direction, I normalized the Z coordinate with the X values. (This was also done when preparing the training dataset, so the model is already acustomed to this input). 

After the coordinates of each point were normalized to the new image dimensions, the values needed to be flattened. Previously, the code used flattened pandas dataframes to generate the output feature vector. This created the following pattern x1, y1, z1, x2, y2, z2, x3, y3, z3 ...
To recreate this, I used a loop and specific index math. The loop checks each point. For each point, the X value is stored using index 3xi, the Y value is stored using index 3xi + 1, and the Z value is stored using index 3xi + 2. Once this was done for the left hand, right hand and pose points, the three 1 dimensional arrays were concatenated to form the output 1-D feature vector. 

Finally, the output vector is checked for nan values. Any nan values are replaced by 0. 

## Wrap up

This post briefly went over the classes and methods used in the real-time sign language word classification program. In the next post I will talk about testing the model and a few finishing touches. Thanks for reading!
