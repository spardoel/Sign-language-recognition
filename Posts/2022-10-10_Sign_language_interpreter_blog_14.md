# Developing a sign language interpreter using machine learning Part 14 - Implementing the new holistic cropping appraoch

In the previous post I talked about how I reworked my appraoch for the program. In the previous version, I used the YOLO model to identify the person in the frame, then running the holistic feature extractor model on the cropped frames. 
In the new version I removed the YOLO model completely. The holistic feature extractor model is run on each frame as soon as it is available. Then, once a clip is ready for processing, the video is cropped and the features are resized to match the cropped frame. 

In this post I will go through the code and talk about how I implemented the new methods. Originally I used pandas dataframes for storing the values for each frame. After some testing I found that the dataframes were slowing things down. 
In a few places in the code, converting the data to numpy arrays was faster. In the end I still used dataframes to pass variables between functions, but when processing speed was important, the data were coppied to numpy arrays immediately before processing. 
This was my compromise between speed and simplicity. I'll talk a bit more about that later. 

## Class overview

Here are the 3 main classes used in the real-time sign recognition program, namely, the VideoCamera class, the FeatureExtractor class and the Data class. 


In the next section I will go through each class and talk about the primary methods. Note that some of the helper methods may be changed in the final version of the code. 
My goal for this post is to talk through my method, the exact code is subject to change. 

## The Video Camera class

Alright, since this program is being made to run on a live webcam feed, I needed some functions to handle getting new frames from the webcam.


Here is a rough outline of the VideoCamera() class. 

![VideoCamera calss](https://user-images.githubusercontent.com/102377660/194667487-28503f9c-35e4-4667-8c7a-9583e2371c95.png)


The VideoCamera class has a CV2 object that controlls the webcam. It also has a function to get the next frame when needed. 

## The FeatureExtractor class

Alright, deep breath. This is the class that holds all the real functionality of the program. Here is a draft of how I envisioned the class. 


![FeatureExtractor calss](https://user-images.githubusercontent.com/102377660/194667992-e63711c0-4eea-42db-bdcf-b7968a368791.png)


Ok, so, this is the class. Let's start with the properties. The class has the holistic model as it's first property, no surprise there since this model is the actual feature extractor. 

Next, there are the pandas dataframes that are used to store the coordinates of the points generated by the holistic model. As mentioned in a previous post, the hands each have 21 points, and each point has a X,Y and Z component. So the shape of these dataframes is 21,3. Similarly, the pose estimator which includes the eyes, shoulder, and arm points has a total of 23 points, each with an X and a Y component. The dataframe used to store these points is of size 23,2. 

The bounding_box property holds the coordinates of the 2 points that define the bounding box for each frame. The frame width and frame height are also saved as properties. 

### The methods

There are many methods within the FeatureExtractor class. Here I will talk through the logic behind each method but I won't necessarily copy paste all of the code. My goal for this section is to explain the flow of data through the program. 

run_feature_extractor_single_frame() is the first method on the list. This method is called for each new frame of data. The method accepts the frame (image) from the VideoCamera class and runs the holistic model on the frame. Then, the 'extract_coordinates_from_results()'  method is called to unpack the results and save them in the left_hand, right_hand, pose properties. Note that this method also multiplies the % values by the image dimensions such that the coordinates of the points are now in pixels. So, instead of the point being at (0.5,0.6) of a 100 by 100 image, the coordinate would now me in the form (50,60). Then, 'get_single_frame_bounding_box()' is called to identify the bounding box that encompasses all of the points held in the left_hand, right_hand and pose properties. The last step in the 'run_feature_extractor_single_frame()' method is to save these values in the 
'Data()' class. I'll talk about that class later. To sumarize, run_feature_extractor_single_frame() runs the image through the holistic model, estracts the coordinates of the points of interest, identifies the bounding box, then saves the values for the current frame. 

For reference here is the code 
```
    def run_feature_extractor_single_frame(self, image, data_structure):

        # reset the variables to zeros
        self.left_hand = pd.DataFrame(np.zeros((21, 3)), columns=["x", "y", "z"])
        self.right_hand = pd.DataFrame(np.zeros((21, 3)), columns=["x", "y", "z"])
        self.pose = pd.DataFrame(np.zeros((23, 2)), columns=["x", "y"])

        # get the shape of the input image
        self.input_image_size_y, self.input_image_size_x, _ = image.shape

        image.flags.writeable = False
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        results = self.holistic.process(image)

        self.__extract_coordinates_from_results(results)

        self.__get_single_frame_bounding_box()

        data_structure.add_new_frame(
            image, self.bounding_box, self.left_hand, self.right_hand, self.pose
        )
```

Next, I want to talk about the 'process_clip()' method. This is where things start to get complicated. This method is called when 50 frames have been processed and it is time for the classification model to guess the sign in the video. 

Here is a simplified version of the code.

```

    def process_clip(self, data_structure):

        (
            frames,
            boxes,
            LH_coordinates,
            RH_coordinates,
            P_coordinates,
        ) = data_structure.get_clip()

        # Find the bounding box min and max values
        self.__get_bounding_box_min_max(boxes)
        self.__check_bounding_box_validity(frames[0])

        # Crop the clip to the bounding box
        clip_cropped = frames[
            :,
            int(self.bounding_box.y_min[0]) : int(self.bounding_box.y_max[0]),
            int(self.bounding_box.x_min[0]) : int(self.bounding_box.x_max[0]),
            :,
        ]

        # loop through the frames
        self.output_features = np.zeros((len(frames), self.num_features))
        ouput_clip = []

        for fr_idx, frame in enumerate(clip_cropped):

            frame = cv2.resize(frame, (350, 350))

            # set the variables to zeros to prevent cary over from previous frame
            self.left_hand = pd.DataFrame(np.zeros((21, 3)), columns=["x", "y", "z"])
            self.right_hand = pd.DataFrame(np.zeros((21, 3)), columns=["x", "y", "z"])
            self.pose = pd.DataFrame(np.zeros((23, 2)), columns=["x", "y"])
            # start_time = time()
            self.__map_coordinates_to_resized_image(
                fr_idx,
                LH_coordinates[fr_idx],
                RH_coordinates[fr_idx],
                P_coordinates[fr_idx],
            )

            # print(f"Time elapsed {time() - start_time}")

            left_index_x = 24
            left_index_y = 25

            # frame = self.__draw_point_on_frame(
            #     frame,
            #     self.output_features[fr_idx][left_index_x],
            #     self.output_features[fr_idx][left_index_y],
            # )
            ouput_clip.append(frame)

        rand = round(10 * random())
        if rand == 7:
            name = (
                "random_video_samples\ random_video_sample_"
                + str(self.random_vid_counter)
                + ".mp4"
            )
            # self.write_video_to_file(ouput_clip, filename=name)
            self.random_vid_counter += 1

        frame_masks = np.ones(shape=(len(frames)), dtype="bool")

        # reset the bounding box to zeros after processing the clip.
        self.bounding_box = pd.DataFrame(
            np.zeros((1, 4)), columns=["x_min", "y_min", "x_max", "y_max"]
        ).astype(int)

        return self.output_features, frame_masks, ouput_clip
```


The method starts by getting the video, lists of coordinates and list of bounding boxes from the 'Data()' class. Then the 'get_bounding_box_min_max()' method is called. This method accepts the list of bounding boxes (one for each frame) and finds the largest bounding box. Each bounding box is defined by 2 points. The upper left point, which is closest to the origin and has the (xmin,ymin) components, and the lower right point with the (xmax,ymax) components. The 'get_bounding_box_min_max()' method finds the minimum and maximum values of the X and Y components separately. Once the xmin,ymin,xmax,ymax values are found, the 'check_new_bounding_box_values()' method is called. This method checks that the values are not nan, and checks if they are larger than the values from the previous frame which are currently held in the bounding_box property. This check is a reminand of the old code and might be removed in the future. But for now, let's carry on. 

Next, once the single bounding box for the clip has been found, the 'check_bound_box_validity()' method is called. This method checks that the bounding box is not larger than the dimensions of the input frame.

Once the bounding box for the video has been found and checked, clice notation was used to crop the video clip. 

Next, the code loops through each frame of the video, and re-sizes the holistic points to match the new, cropped video. Remember, that the coordinates are currently in pixels because the coordinates of the points were converted when the frame was originally processed. As I was saying, the code loops through each frame. The frame is resized to bbe 350 by 350. Then the 'map_coordinates_to_resized_image()' method is called. This method is where the normalization happens.  I think it is worth taking a closer look at. 

### Re-normalizing the coordinates. 

As I mentioned previously, the coordinates of the points returned from the holistic model are represented as fractions of the image size. 
