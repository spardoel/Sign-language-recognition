# Developing a sign language interpreter using machine learning Part 2 - Building a basic CNN model

In this post I will go through my first attempts at building a convolutional nerural network for the classification of still images of the alphabet in American sign language. 
Full disclosure, I referenced this link (https://www.kaggle.com/code/gpiosenka/asl-f1-score-98) and utilized some of the same methodology.
That said, let's jump in. 

## Assessing the dataset
The whole dataset has somewhere around 140 thousand images. For these first tests, I will use a subset of the dataset to speed up training and thus, preserve my sanity a while longer. 
But I am getting ahead of myself... First, it is good to know how many classes (categories) are in the dataset and how many samples (images) are in each. 

To start I provided the path to the dataset folder. The folder is called American because I am using a dataset of American sign language. 
Then, I used a for loop to print the name of each class folder and the number files it contains.
```
DATASET_PATH = "../../American/"

# Print the number of images in each catagory in the dataset
for label in os.listdir(DATASET_PATH):
    print(str(len(os.listdir(DATASET_PATH + label))) + " " + label + " images")
    
num_classes = len(os.listdir(DATASET_PATH))
print(f"There are {num_classes} classes")
```
After running the code, here is the output.
```
3070 0 images
1570 1 images
1570 2 images
1570 3 images
3070 4 images
3070 5 images
3070 6 images
3070 7 images
3070 8 images
3070 9 images
6070 a images
6070 b images
6070 c images
6070 d images
3070 e images
6070 f images
6070 g images
6070 h images
6807 i images
6124 j images
5488 k images
6494 l images
2924 m images
3968 n images
7294 o images
2566 p images
3590 q images
3538 r images
2374 s images
3076 t images
3244 u images
3926 v images
2086 w images
2330 x images
2454 y images
2218 z images
There are 36 classes
```
As you can see there are a lot of classes (categories); 36 to be exact. 26 for the alphabet and 10 for numbers 0-9.
Each class has a few thousand images, but the number of images is not equal between classes. This can lead to issues. 
Giving a model more samples of a certain class can cause bias since the model can 'have more practice' on some classes. This issue is known as class imbalance.
More on that in a minute. 

### Load the image paths and labels

To be able to manipulate the images and to easily create sub-samples, the image paths and labels were loaded into a pandas dataframe. 
During this process, 100 images from each class are kept. This value will be increased in the future, but to keep things fast, the classes are kept small.
Here is the code. 
```
# Create pandas dataframes containing the location and class of each file
image_paths = []
image_labels = []
NUM_IMAGES_PER_CLASS = 100  # The number of images to use from each class

# Check each folder in the dataset. Each one corresponds to a class (category)
for category in os.listdir(DATASET_PATH):
    # for each category folder create folder path
    category_path = os.path.join(DATASET_PATH, category)

    # randomly select (without replacement) a subset of the images in the current class folder
    sample_images = sample(os.listdir(category_path), NUM_IMAGES_PER_CLASS)

    # for each image in the category subset
    for image in sample_images:
        # Add the image name to the path and append the image path and label to their respective lists
        image_paths.append(os.path.join(category_path, image))
        image_labels.append(category)


# combine the lists into a pandas dataframe. First convert to pandas Series and then concatenate.
df = pd.concat(
    [
        pd.Series(image_paths, name="image_paths"),
        pd.Series(image_labels, name="image_labels"),
    ],
    axis=1,
)


# count the times each label (category) appears
print(df["image_labels"].value_counts())

```
This code first declares 2 empty lists that will be used later, and also declares (as a constant) the number of samples to keep from each class (100 for now). 
Then, the code gets the list of folders in the dataset's main folder and loop through them. For each class, the name of the class folder is joined with the dataset path.
Next, the code gets the list of images in the class folder and randomly selects a subset. 
Looping through this subset, the code then creates the path for each image and appends the image path and image label to the image_paths, and image_labels lists.
When this code was run, here was the output.
```
0    100
1    100
k    100
l    100
m    100
n    100
o    100
p    100
q    100
r    100
s    100
t    100
u    100
v    100
w    100
x    100
y    100
j    100
i    100
h    100
8    100
2    100
3    100
4    100
5    100
6    100
7    100
9    100
g    100
a    100
b    100
c    100
d    100
e    100
f    100
z    100
Name: image_labels, dtype: int64
100
```
Great. It looks like each class has exactly 100 images. 
Since we declared the number of samples to keep as a constant, we can easily come back and change this value later to select a larger subset of data. 

## Train / test split
When building a machine learning model, it is good practice to split the available data into training and testing datasets. 
The training data is used to train the model, and the trained model is evaluated using the test data. 
These datasets should be kept separate so that when the model is tested it is being evaluated using samples it has never seen before. 
Importantly, the test data should only be used once, at the very end of the model development process. The whole point of using a separate test set is so to have a representative dataset that your model has never seen before. 
But if you repeatedly train a model, test it on the test set then tweak the model to improve the performance on the test set, well then the test set ceases to be separate and is being used to influence the training of the model. 
This can lead to over fitting since you are tuning the model so that it performs well on the test data. 

Now, you may be asking yourself, how can I tune my model if I can't test it? Well, the answer to that is by using a validation dataset. The validation is basically the 'test' data that is used during model development. 
As the model is trained and adjusted the model performance is monitored using the validation data.

Although the exact proportions can vary, common ratios are 60% training, 20% testing, and 20% validation. These ratios are equivalent to removing 30% of the data for testing, then spliting the remaining data using a 75/25 ratio (as in the code below).
To split the data, the scikit-learn's train_test_split() function is used. 
Here is the code.
```
# use 80% of the data for training. Within that training set, use 20% for validation and 20% for testing.
train_split = 0.8
val_split = 0.25

# use scikit learn's train test split function to generate testing data
intermediate_df, test_df = train_test_split(
    df, train_size=train_split, shuffle=True, random_state=123
)
# from the remaining data, generate the training and validation sets
train_df, valid_df = train_test_split(
    intermediate_df, train_size=1 - val_split, shuffle=True, random_state=123
)

# Print the number of samples in each set to confirm the intended proportions
print(
    f"Training samples: {len(train_df)}, Test samples: {len(test_df)}, Validation samples: {len(valid_df)}"
)

```
The code first defines the ratios to use when splitting the data. Then the data is split into a test set and an intermediate dataset that is then split into the training and validation sets. 
Here is the result of running this section of code.

```
Training samples: 2160, Test samples: 720, Validation samples: 720
```
Alright. So the data has been split into the different sets we will need. 
Well actually, the 'data' has not been split. Rather, dataframe holding the location of the images has been split into different sections. 
To train the model we will need to load the actual images for the model to use, that is where image generators come in. 

## Image Generators
Tensorflow Image Generators are an important part of this project. The image generators load and prepare images that are then passed to the model during training. 
A variety of pre-processing steps can be added to generators making them simple and powerful tools for image classification models. 

First, I declare the image size. The size is defined by the image width and height in pixels and the number of channels per pixel. Since the images are in colour, each image has a red, green and blue value (rgb). Hence, colour images have 3 channels.
The number of images per batch is also defined. During training the model will go through all of the data in the training set multiple times. An epoch refers to the model seeing all of the training data once. 
So if there are 2160 images in our training set, the model will see all 2160 images each epoch. Now, loading that many images into memory at once may not be possible, so batches of images can be used. 
Batching presents the images to the model in smaller groups. After seeing each batch the model weights are updated before starting the next batch. For this reason, the batch size can have an effect on model generalization. 
The ideal batch size is a complicated topic. The size of the batch can affect convergence speed and model generalization. For now, I set the batch size to 80. Keep in mind this is a tunable parameter that can be changed and experimented with during model development. 

```
HEIGHT = 50  # height of each image in pixels
WIDTH = 50  # width of each image in pixels
CHANNELS = 3  # number of channels per pixel, 3 for rgb (because pictures are in colour)
BATCH_SIZE = 80  # number of images per batch
```
With the image size and batch size constants defined, the imagegenerators were created. After being initantiated, the specific parameters of the generators were set. Check out the following code.
```
# create the data generators
gen_train = ImageDataGenerator()
gen_test = ImageDataGenerator()
gen_val = ImageDataGenerator()

# Configure the generators
train_generator = gen_train.flow_from_dataframe(
    train_df,
    x_col="image_paths",
    y_col="image_labels",
    target_size=(HEIGHT, WIDTH),
    class_mode="categorical",
    color_mode="rgb",
    shuffle=False,
    batch_size=BATCH_SIZE,
)
test_generator = gen_test.flow_from_dataframe(
    test_df,
    x_col="image_paths",
    y_col="image_labels",
    target_size=(HEIGHT, WIDTH),
    class_mode="categorical",
    color_mode="rgb",
    shuffle=False,
    batch_size=BATCH_SIZE,
)
validation_generator = gen_val.flow_from_dataframe(
    valid_df,
    x_col="image_paths",
    y_col="image_labels",
    target_size=(HEIGHT, WIDTH),
    class_mode="categorical",
    color_mode="rgb",
    shuffle=False,
    batch_size=BATCH_SIZE,
)
```
As you can see, several parameters are set for each image generator. First, the dataframe is passed along with the column names corresponding to the x and y data. By convention, the x data is the data being classified (in this case images) and the y data is the category label.
Next, the image size is set. The classification mode is set to 'categorical' the input data (in our case this is a list of image locations and labels), the 
