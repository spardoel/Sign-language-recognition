# Developing a sign language interpreter using machine learning Part 13 - Refactoring to improve program speed

As a refresher, here is a video of the current sign language word identification program. 

https://user-images.githubusercontent.com/102377660/193940632-c7080bd4-08bb-46d8-9c6b-0f3fbec07b88.mov

As you can see, the video freezes for a few seconds before the next guess is printed in the top left corner of the frame. I wanted to eliminate this delay. 
This post will be about the process of refactoring the code and how I needed to generate a new dataset and train a whole new model in order to eliminate this delay. 

## What is taking so long? 

I wanted to speed up the code. Well that's great, but first I needed to identify which parts of the code where slowing down the program. I imported the time module and checked various sections of the code. 

In the first draft of this section, I started by copying out the main function that runs the sign language interpreter program. I then used the time module to evaluate the methods and follow the rabbit trail until I identified the part of the code that was taking the longest. After some thought, I removed all that. 

Here is what you need to know. For each frame, the YOLO model was used to identify the person in the frame. This was done as the frames were received. The YOLO model was fast enough to run as the frames were received. Therefore, the YOLO model was not the cause of the delay. As the program ran it collected frames. Once 50 frames of video were ready, then the features were extracted. The features were then passed to the classification model. The classification model ran once per clip of 50 frames. The classification model could be simplified a bit, but compared to the YOLO and feature extraction models, the classification model was basically instantaneous. No, the YOLO and classification models were not the problem. The part of the code responsible for the several second delay before each new classifier decision was the feature extraction model.

### Improving the speed of the feature extraction

The feature extraction step was by far the slowest. Well darn, what to do about that? The feature extraction is pretty vital. In fact, adding this feature extraction model improved perforamnce compared to the previous feature extraction model. I couldn't really just remove the feature extraction model... So, what to do? 

Let's condsider the feature extraction model itself. Running the feature extractor model once didn't take long. After all, mediapipe created the holistic model (which I was using as a feature extractor) to be run in real time. The problem was that I was waiting until 50 frames were ready then running the feature extractor 50 times back to back. **That** is what caused the delay. Well, could I run the feature extractor on each new frame as it is received? Not really, because I needed to wait for the entire clip to be ready before I could crop it, and it was these cropped frames that were given to the feature extractor. hmmm 

### To crop or not to crop?

Ok, so cropping the frames imposed some limitations. Well, is cropping actually necessary? Could the solution be as simple as not cropping? I tried removing the cropping and running the feature ectractor model on the un-cropped frames. The model performed much worse than the cropped videos. It seemed that the best performance was acheived when the video was cropped. This made sense to me because of how the features were represented. The mediapipe holistic model outputs the positional coordinates of the various body parts as percentages of the width and height of the frame. If the frames are all cropped to include only the person, then the magnitude of these percentages matters. For instance, if the frame is cropped to the top of the person's head, the fact that the hand is near the top of the frame is useful information because we know its position relative to the head. However, if the frames are not consistently cropped, then whether the hand is close to the top of the frame or not is meaningless. To a certain extent, these positional relationships can be inferred from the other body landmarks that are generated by the holistic model. However, the holistic model isn't perfect and it often looses track of the hands and other body parts. The position sof the point relative to the frame on the other hand, is not subject to the same random dropout. So, while it may be possible to produce a good model without cropping the frame, it seemed that with my current model I would get the best results if the frames were cropped. So if the frames need to be cropped, then was I stuck? Did I need to simply accept the delay as an unavoidable fact? Well not necessarily. 

The purpose of cropping the video was to ensure that all relevant points are visible and that the person is consistently in the center (ish) of the frame. 
In this case, the relevant points are the coordinates of the body parts as identified by the holistic model. 
With that in mind, what if I ran the holistic model then cropped the frame to be the smallest rectangle possible that contained all the points? 
This would mean that running the YOLO model to find the person within the frame wouldn't be necessary at all. Ok, that's good, but what about the image vs video cropping? 

### When to crop the frames?

In a previous post, I talked about how cropping each individual frame resulted in weird compressions as each frame was resized a different amount. 
For instance, if I moved my arm out to the side to perform a certain sign, the cropped and resized image would be much more compressed horizontally than the previous and following frames. 

Here is the example from my previous post. 

https://user-images.githubusercontent.com/102377660/194618727-e5112a7f-23bf-4d6a-91db-29eb94edec2a.mov


To solve this issue, I kept track of the bounding box of each frame, but didn't immediately crop the frames. 
Instead, I waited until a clip of 50 frames was ready, then cropped the video using a single bounding box that encompassed all other boxes. (One box to rule them all!) 
This ensured that the frames were resized equally. 

Now, back to the current code. If I ran the holistic model on each new (uncropped) frame of data, the coordinates of the body parts would be given as percentages of the current frame size. 
Then, once the clip is ready, and the video is cropped, the percentage values would all be wrong. To address this new issue, I needed to re-scale the coordinates. 

## The new plan

Ok, so I had a plan. 

First, get rid of the YOLO model. It was no longer needed. Instead use the holisitc feature extractor for cropping.

For each frame,
1. Set up the holistic model to locate the body lankmarks
2. Convert the landmark coordinates from % of frame dimensions to actual pixel coordinates
3. Generate a bounding box that encompassed all body parts within the frame

Next, once 50 frames were ready,
1. Find the minimums and maximums of the bounding box corners
2. Generate the largest possible bounding box using the minimum and maximum values (a box that encompasses all bounding boxes from all of the frames)
3. Use the new bounding box to crop the video
4. Re-normalize the body landmark positions as % of the width and height of the new, cropped video 
5. Pass these landmark positions to the classification model as the features. 

Wow, ok, this was turning into a whole big thing. Honestly, I was having a hard time keeping all this straight in my head. 
Visualizing processes always helps me work through problems, so I started working on some class diagrams to help me think. But first, I want to present the logic behind the re-normalization method I used. 

## Cropping and re-normalization

Take a look at the figure below. The top left pannel (A) shows a 640 pixel by 480 pixel image with a point of interest at coordinates (0.3,0.6). These coordinates are the fraction of the the frame width and height. This is the way the holistic model outputs the coordinates of the points. Ok, great. Pannel B shows how this point could also be represented in pixel value by multiplying 0.3 x 640 and 0.6 x 480 which gives the same point with coordinates (192,288), in pixels. This transformation is done for each point of interest every time a new frame is processed. Panel C shows how two points are used to define the bounding box. For this example, the bounding box has points (55,40) and (424,465) in pixels.

![cropping explanation figure 1](https://user-images.githubusercontent.com/102377660/194641285-00110c4d-0b44-4422-9069-5178552e32d4.png)

Now take a look at this next image. Panel D is coppied from the previous image. Panel E shows the image after it has been cropped and the point coordinates adjusted to the coordinate system of the new frame. The coordinates of the point previously were (192,288). The x_min and y_min values are subtracted from the point coordinates to give the coordinates of the point in the new (cropped) image. The new coordinates of the point are (192 - 55, 288 - 40) which gives (137, 248). Also notice the width and height of the cropped image. These values were obtained by subtracting the coordinates of x_min,y_min from x_max,y_max, so 424 - 55 = 369, 465 - 40 = 425 which gives. Now that the coordinates of the point are known in the new coordinate system, normaliz by the image width and height to get the fractional coordinates (137/369, 248/425) which gives (0.37, 0.58). Now when the images are resized to be 350 by 350, the position of tthe point is still valid. 

![cropping explanation figure 2](https://user-images.githubusercontent.com/102377660/194643027-ebfd61da-79d8-4bc0-a975-c73870391c02.png)



## The Video Camera class

Alright, since this program is being made to run on a live webcam feed, I needed some functions to handle getting new frames from the webcam.


Here is a rough outline of the VideoCamera() class. 

![VideoCamera calss](https://user-images.githubusercontent.com/102377660/194667487-28503f9c-35e4-4667-8c7a-9583e2371c95.png)


The VideoCamera class has a CV2 object that controlls the webcam. It also has a function to get the next frame when needed. 

## The FeatureExtractor class

Alright, deep breath. This is the class that holds all the real functionality of the program. Here is a draft of how I envisioned the class. 


![FeatureExtractor calss](https://user-images.githubusercontent.com/102377660/194667992-e63711c0-4eea-42db-bdcf-b7968a368791.png)


Ok, so, this is the class. Let's start with the properties. The class has the holistic model as it's first property, no surprise there since this model is the actual feature extractor. 

Next, there are the pandas dataframes that are used to store the coordinates of the points generated by the holistic model. As mentioned in a previous post, the hands each have 21 points, and each point has a X,Y and Z component. So the shape of these dataframes is 21,3. Similarly, the pose estimator which includes the eyes, shoulder, and arm points has a total of 23 points, each with an X and a Y component. The dataframe used to store these points is of size 23,2. 

The bounding_box property holds the coordinates of the 2 points that define the bounding box for each frame. The frame width and frame height are also saved as properties. 

### The methods

There are many methods within the FeatureExtractor class. Here I will talk through the logic behind each method but I won't necessarily copy paste all of the code. My goal for this section is to explain the flow of data through the program. 

run_feature_extractor_single_frame() is the first method on the list. This method is called for each new frame of data. The method accepts the frame (image) from the VideoCamera class and runs the holistic model on the frame. Then, the 'extract_coordinates_from_results()'  method is called to unpack the results and save them in the left_hand, right_hand, pose properties. Note that this method also multiplies the % values by the image dimensions such that the coordinates of the points are now in pixels. So, instead of the point being at (0.5,0.6) of a 100 by 100 image, the coordinate would now me in the form (50,60). Then, 'get_single_frame_bounding_box()' is called to identify the bounding box that encompasses all of the points held in the left_hand, right_hand and pose properties. The last step in the 'run_feature_extractor_single_frame()' method is to save these values in the 
'Data()' class. I'll talk about that class later. To sumarize, run_feature_extractor_single_frame() runs the image through the holistic model, estracts the coordinates of the points of interest, identifies the bounding box, then saves the values for the current frame. 

For reference here is the code 
```
    def run_feature_extractor_single_frame(self, image, data_structure):

        # reset the variables to zeros
        self.left_hand = pd.DataFrame(np.zeros((21, 3)), columns=["x", "y", "z"])
        self.right_hand = pd.DataFrame(np.zeros((21, 3)), columns=["x", "y", "z"])
        self.pose = pd.DataFrame(np.zeros((23, 2)), columns=["x", "y"])

        # get the shape of the input image
        self.input_image_size_y, self.input_image_size_x, _ = image.shape

        image.flags.writeable = False
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        results = self.holistic.process(image)

        self.__extract_coordinates_from_results(results)

        self.__get_single_frame_bounding_box()

        data_structure.add_new_frame(
            image, self.bounding_box, self.left_hand, self.right_hand, self.pose
        )
```

Next, I want to talk about the 'process_clip()' method. This is where things start to get complicated. This method is called when 50 frames have been processed and it is time for the classification model to guess the sign in the video. 

Here is a simplified version of the code.

```

    def process_clip(self, data_structure):

        (
            frames,
            boxes,
            LH_coordinates,
            RH_coordinates,
            P_coordinates,
        ) = data_structure.get_clip()

        # Find the bounding box min and max values
        self.__get_bounding_box_min_max(boxes)
        self.__check_bounding_box_validity(frames[0])

        # Crop the clip to the bounding box
        clip_cropped = frames[
            :,
            int(self.bounding_box.y_min[0]) : int(self.bounding_box.y_max[0]),
            int(self.bounding_box.x_min[0]) : int(self.bounding_box.x_max[0]),
            :,
        ]

        # loop through the frames
        self.output_features = np.zeros((len(frames), self.num_features))
        ouput_clip = []

        for fr_idx, frame in enumerate(clip_cropped):

            frame = cv2.resize(frame, (350, 350))

            # set the variables to zeros to prevent cary over from previous frame
            self.left_hand = pd.DataFrame(np.zeros((21, 3)), columns=["x", "y", "z"])
            self.right_hand = pd.DataFrame(np.zeros((21, 3)), columns=["x", "y", "z"])
            self.pose = pd.DataFrame(np.zeros((23, 2)), columns=["x", "y"])
            # start_time = time()
            self.__map_coordinates_to_resized_image(
                fr_idx,
                LH_coordinates[fr_idx],
                RH_coordinates[fr_idx],
                P_coordinates[fr_idx],
            )

            # print(f"Time elapsed {time() - start_time}")

            left_index_x = 24
            left_index_y = 25

            # frame = self.__draw_point_on_frame(
            #     frame,
            #     self.output_features[fr_idx][left_index_x],
            #     self.output_features[fr_idx][left_index_y],
            # )
            ouput_clip.append(frame)

        rand = round(10 * random())
        if rand == 7:
            name = (
                "random_video_samples\ random_video_sample_"
                + str(self.random_vid_counter)
                + ".mp4"
            )
            # self.write_video_to_file(ouput_clip, filename=name)
            self.random_vid_counter += 1

        frame_masks = np.ones(shape=(len(frames)), dtype="bool")

        # reset the bounding box to zeros after processing the clip.
        self.bounding_box = pd.DataFrame(
            np.zeros((1, 4)), columns=["x_min", "y_min", "x_max", "y_max"]
        ).astype(int)

        return self.output_features, frame_masks, ouput_clip
```


The method starts by getting the video, lists of coordinates and list of bounding boxes from the 'Data()' class. Then the 'get_bounding_box_min_max()' method is called. This method accepts the list of bounding boxes (one for each frame) and finds the largest bounding box. Each bounding box is defined by 2 points. The upper left point, which is closest to the origin and has the (xmin,ymin) components, and the lower right point with the (xmax,ymax) components. The 'get_bounding_box_min_max()' method finds the minimum and maximum values of the X and Y components separately. Once the xmin,ymin,xmax,ymax values are found, the 'check_new_bounding_box_values()' method is called. This method checks that the values are not nan, and checks if they are larger than the values from the previous frame which are currently held in the bounding_box property. This check is a reminand of the old code and might be removed in the future. But for now, let's carry on. 

Next, once the single bounding box for the clip has been found, the 'check_bound_box_validity()' method is called. This method checks that the bounding box is not larger than the dimensions of the input frame.

Once the bounding box for the video has been found and checked, clice notation was used to crop the video clip. 

Next, the code loops through each frame of the video, and re-sizes the holistic points to match the new, cropped video. Remember, that the coordinates are currently in pixels because the coordinates of the points were converted when the frame was originally processed. As I was saying, the code loops through each frame. The frame is resized to bbe 350 by 350. Then the 'map_coordinates_to_resized_image()' method is called. This method is where the normalization happens.  I think it is worth taking a closer look at. 

### Re-normalizing the coordinates. 

As I mentioned previously, the coordinates of the points returned from the holistic model are represented as fractions of the image size. 

